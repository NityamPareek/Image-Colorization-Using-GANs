{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-02T13:14:28.360217Z","iopub.status.busy":"2023-06-02T13:14:28.359518Z","iopub.status.idle":"2023-06-02T13:14:32.030532Z","shell.execute_reply":"2023-06-02T13:14:32.029611Z","shell.execute_reply.started":"2023-06-02T13:14:28.360183Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import time\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:14:32.033098Z","iopub.status.busy":"2023-06-02T13:14:32.032493Z","iopub.status.idle":"2023-06-02T13:18:32.365175Z","shell.execute_reply":"2023-06-02T13:18:32.364098Z","shell.execute_reply.started":"2023-06-02T13:14:32.033065Z"},"trusted":true},"outputs":[],"source":["!pip install fastai\n","from fastai.data.external import untar_data, URLs\n","coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","use_colab = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:32.367851Z","iopub.status.busy":"2023-06-02T13:18:32.367228Z","iopub.status.idle":"2023-06-02T13:18:32.502892Z","shell.execute_reply":"2023-06-02T13:18:32.501714Z","shell.execute_reply.started":"2023-06-02T13:18:32.367816Z"},"trusted":true},"outputs":[],"source":["paths = glob.glob(coco_path + \"/*.jpg\") # Grabbing all the image file names\n","np.random.seed(123)\n","paths_subset = np.random.choice(paths, 10000, replace=False) # choosing 10000 images randomly\n","rand_idxs = np.random.permutation(10000)\n","train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n","val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n","train_paths = paths_subset[train_idxs]\n","val_paths = paths_subset[val_idxs]\n","print(len(train_paths), len(val_paths))\n","\n","np.savetxt(\"train_paths.csv\", train_paths, fmt = \"%s\")\n","np.savetxt(\"val_paths.csv\", val_paths, fmt = \"%s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:32.507526Z","iopub.status.busy":"2023-06-02T13:18:32.507168Z","iopub.status.idle":"2023-06-02T13:18:41.531454Z","shell.execute_reply":"2023-06-02T13:18:41.530345Z","shell.execute_reply.started":"2023-06-02T13:18:32.507493Z"},"trusted":true},"outputs":[],"source":["#Creating Data Loaders\n","SIZE = 256\n","class ColorizationDataset(Dataset):\n","    def __init__(self, paths, split='train'):\n","        if split == 'train':\n","            self.transforms = transforms.Compose([\n","                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n","                transforms.RandomHorizontalFlip(), # A little data augmentation!\n","            ])\n","        elif split == 'val':\n","            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n","        \n","        self.split = split\n","        self.size = SIZE\n","        self.paths = paths\n","    \n","    def __getitem__(self, idx):\n","        img = Image.open(self.paths[idx]).convert(\"RGB\")\n","        img = self.transforms(img)\n","        img = np.array(img)\n","        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n","        img_lab = transforms.ToTensor()(img_lab)\n","        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n","        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n","        \n","        return {'L': L, 'ab': ab}\n","    \n","    def __len__(self):\n","        return len(self.paths)\n","\n","def make_dataloaders(batch_size=8, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n","    dataset = ColorizationDataset(**kwargs)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n","                            pin_memory=pin_memory)\n","    return dataloader\n","\n","train_dl = make_dataloaders(paths=train_paths, split='train')\n","val_dl = make_dataloaders(paths=val_paths, split='val')\n","\n","data = next(iter(train_dl))\n","Ls, abs_ = data['L'], data['ab']\n","print(Ls.shape, abs_.shape)\n","print(len(train_dl), len(val_dl))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Implementation as Proposed by Paper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.170517Z","iopub.status.busy":"2023-06-02T13:18:43.170217Z","iopub.status.idle":"2023-06-02T13:18:43.190402Z","shell.execute_reply":"2023-06-02T13:18:43.189364Z","shell.execute_reply.started":"2023-06-02T13:18:43.170490Z"},"trusted":true},"outputs":[],"source":["#Generator as Proposed by the Paper\n","class UnetBlock(nn.Module):\n","    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n","                 innermost=False, outermost=False):\n","        super().__init__()\n","        self.outermost = outermost\n","        if input_c is None: input_c = nf\n","        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n","                             stride=2, padding=1, bias=False)\n","        downrelu = nn.LeakyReLU(0.2, True)\n","        downnorm = nn.BatchNorm2d(ni)\n","        uprelu = nn.ReLU(True)\n","        upnorm = nn.BatchNorm2d(nf)\n","        \n","        if outermost:\n","            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n","                                        stride=2, padding=1)\n","            down = [downconv]\n","            up = [uprelu, upconv, nn.Tanh()]\n","            model = down + [submodule] + up\n","        elif innermost:\n","            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n","                                        stride=2, padding=1, bias=False)\n","            down = [downrelu, downconv]\n","            up = [uprelu, upconv, upnorm]\n","            model = down + up\n","        else:\n","            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n","                                        stride=2, padding=1, bias=False)\n","            down = [downrelu, downconv, downnorm]\n","            up = [uprelu, upconv, upnorm]\n","            if dropout: up += [nn.Dropout(0.5)]\n","            model = down + [submodule] + up\n","        self.model = nn.Sequential(*model)\n","    \n","    def forward(self, x):\n","        if self.outermost:\n","            return self.model(x)\n","        else:\n","            return torch.cat([x, self.model(x)], 1)\n","\n","class Unet(nn.Module):\n","    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n","        super().__init__()\n","        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n","        for _ in range(n_down - 5):\n","            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n","        out_filters = num_filters * 8\n","        for _ in range(3):\n","            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n","            out_filters //= 2\n","        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n","    \n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.193516Z","iopub.status.busy":"2023-06-02T13:18:43.192567Z","iopub.status.idle":"2023-06-02T13:18:43.206025Z","shell.execute_reply":"2023-06-02T13:18:43.205147Z","shell.execute_reply.started":"2023-06-02T13:18:43.193481Z"},"trusted":true},"outputs":[],"source":["#Discriminator\n","class PatchDiscriminator(nn.Module):\n","    def __init__(self, input_c, num_filters=64, n_down=3):\n","        super().__init__()\n","        model = [self.get_layers(input_c, num_filters, norm=False)]\n","        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n","                          for i in range(n_down)] # the 'if' statement is taking care of not using\n","                                                  # stride of 2 for the last block in this loop\n","        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n","                                                                                             # activation for the last layer of the model\n","        self.model = nn.Sequential(*model)                                                   \n","        \n","    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n","        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n","        if norm: layers += [nn.BatchNorm2d(nf)]\n","        if act: layers += [nn.LeakyReLU(0.2, True)]\n","        return nn.Sequential(*layers)\n","    \n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.207848Z","iopub.status.busy":"2023-06-02T13:18:43.207434Z","iopub.status.idle":"2023-06-02T13:18:43.217922Z","shell.execute_reply":"2023-06-02T13:18:43.217189Z","shell.execute_reply.started":"2023-06-02T13:18:43.207811Z"},"trusted":true},"outputs":[],"source":["#GAN Loss Function\n","class GANLoss(nn.Module):\n","    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n","        super().__init__()\n","        self.register_buffer('real_label', torch.tensor(real_label))\n","        self.register_buffer('fake_label', torch.tensor(fake_label))\n","        if gan_mode == 'vanilla':\n","            self.loss = nn.BCEWithLogitsLoss()\n","        elif gan_mode == 'lsgan':\n","            self.loss = nn.MSELoss()\n","    \n","    def get_labels(self, preds, target_is_real):\n","        if target_is_real:\n","            labels = self.real_label\n","        else:\n","            labels = self.fake_label\n","        return labels.expand_as(preds)\n","    \n","    def __call__(self, preds, target_is_real):\n","        labels = self.get_labels(preds, target_is_real)\n","        loss = self.loss(preds, labels)\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.219999Z","iopub.status.busy":"2023-06-02T13:18:43.219127Z","iopub.status.idle":"2023-06-02T13:18:43.234469Z","shell.execute_reply":"2023-06-02T13:18:43.233725Z","shell.execute_reply.started":"2023-06-02T13:18:43.219968Z"},"trusted":true},"outputs":[],"source":["def init_weights(net, init='norm', gain=0.02):\n","    \n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and 'Conv' in classname:\n","            if init == 'norm':\n","                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n","            elif init == 'xavier':\n","                nn.init.xavier_normal_(m.weight.data, gain=gain)\n","            elif init == 'kaiming':\n","                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","            \n","            if hasattr(m, 'bias') and m.bias is not None:\n","                nn.init.constant_(m.bias.data, 0.0)\n","        elif 'BatchNorm2d' in classname:\n","            nn.init.normal_(m.weight.data, 1., gain)\n","            nn.init.constant_(m.bias.data, 0.)\n","            \n","    net.apply(init_func)\n","    print(f\"model initialized with {init} initialization\")\n","    return net\n","\n","def init_model(model, device):\n","    model = model.to(device)\n","    model = init_weights(model)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.238689Z","iopub.status.busy":"2023-06-02T13:18:43.238088Z","iopub.status.idle":"2023-06-02T13:18:43.255033Z","shell.execute_reply":"2023-06-02T13:18:43.254125Z","shell.execute_reply.started":"2023-06-02T13:18:43.238641Z"},"trusted":true},"outputs":[],"source":["class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.count, self.avg, self.sum = [0.] * 3\n","    \n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += count * val\n","        self.avg = self.sum / self.count\n","\n","def create_loss_meters():\n","    loss_D_fake = AverageMeter()\n","    loss_D_real = AverageMeter()\n","    loss_D = AverageMeter()\n","    loss_G_GAN = AverageMeter()\n","    loss_G_L1 = AverageMeter()\n","    loss_G = AverageMeter()\n","    \n","    return {'loss_D_fake': loss_D_fake,\n","            'loss_D_real': loss_D_real,\n","            'loss_D': loss_D,\n","            'loss_G_GAN': loss_G_GAN,\n","            'loss_G_L1': loss_G_L1,\n","            'loss_G': loss_G}\n","\n","def update_losses(model, loss_meter_dict, count):\n","    for loss_name, loss_meter in loss_meter_dict.items():\n","        loss = getattr(model, loss_name)\n","        loss_meter.update(loss.item(), count=count)\n","\n","def lab_to_rgb(L, ab):\n","    \"\"\"\n","    Takes a batch of images\n","    \"\"\"\n","    \n","    L = (L + 1.) * 50.\n","    ab = ab * 110.\n","    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n","    rgb_imgs = []\n","    for img in Lab:\n","        img_rgb = lab2rgb(img)\n","        rgb_imgs.append(img_rgb)\n","    return np.stack(rgb_imgs, axis=0)\n","    \n","def visualize(model, data, save=True):\n","    model.net_G.eval()\n","    with torch.no_grad():\n","        model.setup_input(data)\n","        model.forward()\n","    model.net_G.train()\n","    fake_color = model.fake_color.detach()\n","    real_color = model.ab\n","    L = model.L\n","    fake_imgs = lab_to_rgb(L, fake_color)\n","    real_imgs = lab_to_rgb(L, real_color)\n","    fig = plt.figure(figsize=(15, 8))\n","    for i in range(5):\n","        ax = plt.subplot(3, 5, i + 1)\n","        ax.imshow(L[i][0].cpu(), cmap='gray')\n","        ax.axis(\"off\")\n","        ax = plt.subplot(3, 5, i + 1 + 5)\n","        ax.imshow(fake_imgs[i])\n","        ax.axis(\"off\")\n","        ax = plt.subplot(3, 5, i + 1 + 10)\n","        ax.imshow(real_imgs[i])\n","        ax.axis(\"off\")\n","    plt.show()\n","    if save:\n","        fig.savefig(f\"colorization_{time.time()}.png\")\n","        \n","def log_results(loss_meter_dict):\n","    for loss_name, loss_meter in loss_meter_dict.items():\n","        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.256861Z","iopub.status.busy":"2023-06-02T13:18:43.256525Z","iopub.status.idle":"2023-06-02T13:18:43.274687Z","shell.execute_reply":"2023-06-02T13:18:43.273778Z","shell.execute_reply.started":"2023-06-02T13:18:43.256831Z"},"trusted":true},"outputs":[],"source":["#Compiling the Model\n","class MainModel(nn.Module):\n","    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n","                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n","        super().__init__()\n","        \n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.lambda_L1 = lambda_L1\n","        \n","        if net_G is None:\n","            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n","        else:\n","            self.net_G = net_G.to(self.device)\n","        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n","        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n","        self.L1criterion = nn.L1Loss()\n","        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n","        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n","    \n","    def set_requires_grad(self, model, requires_grad=True):\n","        for p in model.parameters():\n","            p.requires_grad = requires_grad\n","        \n","    def setup_input(self, data):\n","        self.L = data['L'].to(self.device)\n","        self.ab = data['ab'].to(self.device)\n","        \n","    def forward(self):\n","        self.fake_color = self.net_G(self.L)\n","    \n","    def backward_D(self):\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","        fake_preds = self.net_D(fake_image.detach())\n","        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n","        real_image = torch.cat([self.L, self.ab], dim=1)\n","        real_preds = self.net_D(real_image)\n","        self.loss_D_real = self.GANcriterion(real_preds, True)\n","        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n","        self.loss_D.backward()\n","    \n","    def backward_G(self):\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","        fake_preds = self.net_D(fake_image)\n","        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n","        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n","        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n","        self.loss_G.backward()\n","    \n","    def optimize(self):\n","        self.forward()\n","        self.net_D.train()\n","        self.set_requires_grad(self.net_D, True)\n","        self.opt_D.zero_grad()\n","        self.backward_D()\n","        self.opt_D.step()\n","        \n","        self.net_G.train()\n","        self.set_requires_grad(self.net_D, False)\n","        self.opt_G.zero_grad()\n","        self.backward_G()\n","        self.opt_G.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.276497Z","iopub.status.busy":"2023-06-02T13:18:43.276038Z","iopub.status.idle":"2023-06-02T13:18:43.290088Z","shell.execute_reply":"2023-06-02T13:18:43.289067Z","shell.execute_reply.started":"2023-06-02T13:18:43.276465Z"},"trusted":true},"outputs":[],"source":["#Training Function\n","def train_model(model, train_dl, epochs):\n","    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n","    for e in range(epochs):\n","        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to \n","        i = 0                                  # log the losses of the complete network\n","        for data in tqdm(train_dl):\n","            model.setup_input(data) \n","            model.optimize()\n","            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n","            i += 1\n","            if ((e+1) % 10 == 0 or e==0) and i==500:\n","                print(f\"\\nEpoch {e+1}/{epochs}\")\n","                #print(f\"Iteration {i}/{len(train_dl)}\")\n","                log_results(loss_meter_dict) # function to print out the losses\n","                visualize(model, data, save=False) # function displaying the model's outputs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training the GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T13:18:43.291851Z","iopub.status.busy":"2023-06-02T13:18:43.291336Z","iopub.status.idle":"2023-06-02T14:16:27.715952Z","shell.execute_reply":"2023-06-02T14:16:27.714889Z","shell.execute_reply.started":"2023-06-02T13:18:43.291819Z"},"trusted":true},"outputs":[],"source":["base_model = MainModel()\n","train_model(base_model, train_dl, 20)\n","torch.save(base_model.state_dict(), \"base-GAN.pt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pre-Training the U-Net Generator with ResNet Backbone"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:24:32.009455Z","iopub.status.busy":"2023-06-02T14:24:32.008594Z","iopub.status.idle":"2023-06-02T14:24:32.018725Z","shell.execute_reply":"2023-06-02T14:24:32.017446Z","shell.execute_reply.started":"2023-06-02T14:24:32.009419Z"},"trusted":true},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)\n","        self.relu = nn.ReLU(inplace=True)\n","    def forward(self, inputs):\n","        x = inputs.clone().detach()\n","        out = self.layer(x)\n","        residual  = self.identity_map(inputs)\n","        skip = out + residual\n","        return self.relu(skip)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:24:37.546059Z","iopub.status.busy":"2023-06-02T14:24:37.545164Z","iopub.status.idle":"2023-06-02T14:24:37.552029Z","shell.execute_reply":"2023-06-02T14:24:37.551004Z","shell.execute_reply.started":"2023-06-02T14:24:37.546016Z"},"trusted":true},"outputs":[],"source":["class DownSampleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            ResBlock(in_channels, out_channels)\n","        )\n","\n","    def forward(self, inputs):\n","        return self.layer(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:24:39.347633Z","iopub.status.busy":"2023-06-02T14:24:39.346542Z","iopub.status.idle":"2023-06-02T14:24:39.354638Z","shell.execute_reply":"2023-06-02T14:24:39.353591Z","shell.execute_reply.started":"2023-06-02T14:24:39.347592Z"},"trusted":true},"outputs":[],"source":["class UpSampleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        \n","        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n","        \n","    def forward(self, inputs, skip):\n","        x = self.upsample(inputs)\n","        x = torch.cat([x, skip], dim=1)\n","        x = self.res_block(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:24:46.331818Z","iopub.status.busy":"2023-06-02T14:24:46.331425Z","iopub.status.idle":"2023-06-02T14:24:46.342403Z","shell.execute_reply":"2023-06-02T14:24:46.341416Z","shell.execute_reply.started":"2023-06-02T14:24:46.331787Z"},"trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):\n","        super().__init__()\n","        self.encoding_layer1_ = ResBlock(input_channel,64)\n","        self.encoding_layer2_ = DownSampleConv(64, 128)\n","        self.encoding_layer3_ = DownSampleConv(128, 256)\n","        self.bridge = DownSampleConv(256, 512)\n","        self.decoding_layer3_ = UpSampleConv(512, 256)\n","        self.decoding_layer2_ = UpSampleConv(256, 128)\n","        self.decoding_layer1_ = UpSampleConv(128, 64)\n","        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n","        self.dropout = nn.Dropout2d(dropout_rate)\n","        \n","    def forward(self, inputs):\n","        ###################### Enocoder #########################\n","        e1 = self.encoding_layer1_(inputs)\n","        e1 = self.dropout(e1)\n","        e2 = self.encoding_layer2_(e1)\n","        e2 = self.dropout(e2)\n","        e3 = self.encoding_layer3_(e2)\n","        e3 = self.dropout(e3)\n","        \n","        ###################### Bridge #########################\n","        bridge = self.bridge(e3)\n","        bridge = self.dropout(bridge)\n","        \n","        ###################### Decoder #########################\n","        d3 = self.decoding_layer3_(bridge, e3)\n","        d2 = self.decoding_layer2_(d3, e2)\n","        d1 = self.decoding_layer1_(d2, e1)\n","        \n","        ###################### Output #########################\n","        output = self.output(d1)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:24:52.165702Z","iopub.status.busy":"2023-06-02T14:24:52.164688Z","iopub.status.idle":"2023-06-02T14:24:52.173220Z","shell.execute_reply":"2023-06-02T14:24:52.172027Z","shell.execute_reply.started":"2023-06-02T14:24:52.165646Z"},"trusted":true},"outputs":[],"source":["def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n","    for e in range(epochs):\n","        loss_meter = AverageMeter()\n","        for data in tqdm(train_dl):\n","            L, ab = data['L'].to(device), data['ab'].to(device)\n","            preds = net_G(L)\n","            loss = criterion(preds, ab)\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","            \n","            loss_meter.update(loss.item(), L.size(0))\n","            \n","        print(f\"Epoch {e + 1}/{epochs}\")\n","        print(f\"L1 Loss: {loss_meter.avg:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T14:27:47.388579Z","iopub.status.busy":"2023-06-02T14:27:47.387768Z","iopub.status.idle":"2023-06-02T15:42:09.675941Z","shell.execute_reply":"2023-06-02T15:42:09.674582Z","shell.execute_reply.started":"2023-06-02T14:27:47.388547Z"},"trusted":true},"outputs":[],"source":["net_G = Generator(1,2).to(device)\n","opt = optim.Adam(net_G.parameters(), lr=1e-4)\n","criterion = nn.L1Loss()        \n","pretrain_generator(net_G, train_dl, opt, criterion, 20)\n","torch.save(net_G.state_dict(), \"res18-unet.pt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training the GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T15:42:09.679169Z","iopub.status.busy":"2023-06-02T15:42:09.678747Z","iopub.status.idle":"2023-06-02T17:24:00.332986Z","shell.execute_reply":"2023-06-02T17:24:00.331901Z","shell.execute_reply.started":"2023-06-02T15:42:09.679126Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","net_G = Generator(1, 2).to(device)\n","net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\n","model = MainModel(net_G=net_G)\n","train_model(model, train_dl, 20)\n","torch.save(model.state_dict(), \"final_model_weights.pt\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}
